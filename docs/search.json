[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "A Deep Dive into PyTorch‚Äôs Autograd Engine\n\n\n\n\n\n\n\nNeural Networks\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nAyush Raj Dahal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/autograd-from-scratch/index.html",
    "href": "posts/autograd-from-scratch/index.html",
    "title": "A Deep Dive into PyTorch‚Äôs Autograd Engine",
    "section": "",
    "text": "In an attempt of understanding the underlying mechanism of PyTorch‚Äôs autograd engine, I created its mini-version that does the same job, but on a much simpler and less-polished manner. This blog post explains the core ideas behind that project. Hopefully, by the end of this post, you will have a good understanding of how the autograd engine works and even how to implement a simple version of it from scratch. So, let‚Äôs get started!"
  },
  {
    "objectID": "posts/autograd-from-scratch/index.html#contextualizing-the-problem",
    "href": "posts/autograd-from-scratch/index.html#contextualizing-the-problem",
    "title": "A Deep Dive into PyTorch‚Äôs Autograd Engine",
    "section": "Contextualizing the Problem",
    "text": "Contextualizing the Problem\nIn order to understand the need of an Autograd engine, we will first need to understand what makes its existence relevant in the first place. Although it is commonly associated with Neural Networks in Deep Learning ‚Äì which is pretty much the only case we will be concerned with as we move forward in our AI journey ‚Äì it is actually a completely independent concept that has nothing to do with Neural Networks, and can be used in applications as broad as weather forecasting and [xyz]. It is the idea of computing derivatives of some variable with reference to all the variables that were used to result in that value.\nIf terms like ‚ÄúNeural Networks‚Äù and ‚ÄúDeep Learning‚Äù doesn‚Äôt make any sense to you yet, don‚Äôt worry about it. As you‚Äôll see in this as well as some of my upcoming blog posts, those concepts are actually a lot, lot simpler than they sound. Together, we will be breaking everything down to the first principles and create everything from scratch. So, stick with me. You‚Äôre in for a fun ride!\n\nNumPy vs.¬†PyTorch\nThings that are similar between the two:\n\nBoth provide a way to create n-dimensional arrays\nBoth provide a way to perform mathematical operations on those arrays\nBoth are fast and optimized, sometimes even by a factor of thousands, as compared to standard Python operations (since Python is a high-level language that needs a lot of memory and CPU cycles to perform simple operations)\n\nThings that separate the two:\n\nParallel runtime & GPU usage\nData type restriction\nWhat is a tensor, tensor vs.¬†Array\n\nThe main one that we‚Äôre concerned with:\n\nAutomatic Differentiation Engine"
  },
  {
    "objectID": "posts/autograd-from-scratch/index.html#references",
    "href": "posts/autograd-from-scratch/index.html#references",
    "title": "A Deep Dive into PyTorch‚Äôs Autograd Engine",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ayush Raj Dahal",
    "section": "",
    "text": "üëãHi there! I am Ayush Raj Dahal.\nI am a Software Developer currently based in New Orleans, USA. I work primarily on Data Science and Web Development-related projects, some of which you can explore on my GitHub. I also sometimes write blog posts explaining stuff I learn or build. You can check them out here.\nPlease feel free to email/message me if you would like to discuss projects!"
  },
  {
    "objectID": "posts/autograd-from-scratch/index.html#contextualizing-the-need-for-automatic-differentiation",
    "href": "posts/autograd-from-scratch/index.html#contextualizing-the-need-for-automatic-differentiation",
    "title": "A Deep Dive into PyTorch‚Äôs Autograd Engine",
    "section": "Contextualizing the Need for Automatic Differentiation",
    "text": "Contextualizing the Need for Automatic Differentiation\nIn order to understand the need for an Autograd engine, we will first need to understand what makes its existence relevant in the first place. The term ‚Äúautograd‚Äù is short for ‚Äúautomatic gradient,‚Äù sometimes also referred to as ‚Äúautomatic differentiation‚Äù. It is the idea of computing derivatives of some variable with reference to all the variables and/or constants that were used to result in that value.\nAlthough it is commonly associated with Neural Networks in Deep Learning ‚Äì which is pretty much the only case we will be concerned with as we move forward in our AI journey ‚Äì it is actually a completely independent concept that has nothing to do with Neural Networks. In fact, it has traditionally been used in applications as broad as weather forecasting, [abc], and [xyz].\nIf terms like ‚ÄúNeural Networks‚Äù and ‚ÄúDeep Learning‚Äù doesn‚Äôt make any sense to you yet, don‚Äôt worry about it. As you‚Äôll see in this as well as some of my upcoming blog posts, those concepts are actually a lot, lot simpler than they sound. Together, we will be breaking everything down to the first principles and create everything from scratch. So, stick with me. You‚Äôre in for a fun ride!\n\nNumPy vs.¬†PyTorch\nThings that are similar between the two:\n\nBoth provide a way to create n-dimensional arrays\n\n\nBoth provide a way to perform mathematical operations on those arrays\nBoth are fast and optimized, sometimes even by a factor of thousands, as compared to standard Python operations (since Python is a high-level language that needs a lot of memory and CPU cycles to perform simple operations)\n\nThings that separate the two:\n\nParallel runtime & GPU usage\nData type restriction\nWhat is a tensor, tensor vs.¬†Array\n\nThe main one that we‚Äôre concerned with:\n\nAutomatic Differentiation Engine"
  }
]