{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Deep Dive into PyTorch's Automatic Differentiation Engine\"\n",
    "author: \"Ayush Raj Dahal\"\n",
    "date: \"2023-10-11\"\n",
    "categories: [Neural Networks, Machine Learning]\n",
    "image: \"image.jpg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an attempt of understanding the underlying mechanism of PyTorch's autograd engine, I created its mini-version that does the same job, but on a much lower scale. This blog post explains the core ideas behind that project. This is mostly just to test my own understanding - yet again. Hopefully, by the end of this post, you will have a better understanding of to implement a simple autograd engine. This post assumes that you have a basic familiarity with Python and a little bit of Calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
